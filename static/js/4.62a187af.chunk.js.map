{"version":3,"sources":["components/blog/20190120/model.png","components/blog/20190120/loss_x_val_loss.png","components/blog/20190120/acc_x_val_acc.png","components/blog/20190120/index.js"],"names":["module","exports","__webpack_require__","p","Post20190119","document","title","this","props","react__WEBPACK_IMPORTED_MODULE_5___default","a","createElement","Fragment","date","href","target","rel","react_highlight__WEBPACK_IMPORTED_MODULE_6___default","className","style","overflowX","src","model","alt","loss","acc","React","Component"],"mappings":"6EAAAA,EAAAC,QAAiBC,EAAAC,EAAuB,uDCAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,iECAxCH,EAAAC,QAAiBC,EAAAC,EAAuB,+PCOnBC,8LAEjBC,SAASC,MAAQ,YAAcC,KAAKC,MAAMF,uCAI1C,OACJG,EAAAC,EAAAC,cAAAF,EAAAC,EAAAE,SAAA,KACAH,EAAAC,EAAAC,cAAA,UAAKJ,KAAKC,MAAMF,OAChBG,EAAAC,EAAAC,cAAA,aAAQJ,KAAKC,MAAMK,MACnBJ,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAAA,uTACkEF,EAAAC,EAAAC,cAAA,sDADlE,SAC0FF,EAAAC,EAAAC,cAAA,0CAD1F,oGAGAF,EAAAC,EAAAC,cAAA,2JAC6CF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,gDAAgDC,OAAO,SAASC,IAAI,uBAA5E,wBAD7C,wnBACyRP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,gGAAgGC,OAAO,SAASC,IAAI,uBAA5H,oDADzR,SACwbP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,sDAAsDC,OAAO,SAASC,IAAI,uBAAlF,gEADxb,mGAC8jBP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,mCAAmCC,OAAO,SAASC,IAAI,uBAA/D,gBAD9jB,kFAIAP,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,mCAAmCC,OAAO,SAASC,IAAI,uBAA/D,8FADF,uOAIAP,EAAAC,EAAAC,cAAA,yCACAF,EAAAC,EAAAC,cAAA,sDACSF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,2CAA2CC,OAAO,SAASC,IAAI,uBAAvE,UADT,sXAGAP,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,uDAAuDC,OAAO,SAASC,IAAI,uBAAnF,aADF,+DACkIP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,+DAA+DC,OAAO,SAASC,IAAI,uBAA3F,sBADlI,8QAC6TP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,6BAA6BC,OAAO,SAASC,IAAI,uBAAzD,UAD7T,sEAGAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,QAArB,yDAIAT,EAAAC,EAAAC,cAAA,SACEF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,uBAAuBC,OAAO,SAASC,IAAI,uBAAnD,oBADF,2JAGAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,QAArB,uDAKAT,EAAAC,EAAAC,cAAA,iEACAF,EAAAC,EAAAC,cAAA,gJACwBF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,wCAAwCC,OAAO,SAASC,IAAI,uBAApE,sBADxB,oJAGAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,gHAOAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SACxBX,EAAAC,EAAAC,cAAA,SAAOO,UAAU,aACfT,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAAA,yBACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,kBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,mBACAF,EAAAC,EAAAC,cAAA,mBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,kBACAF,EAAAC,EAAAC,cAAA,mBACAF,EAAAC,EAAAC,cAAA,wBAGJF,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,qCACAF,EAAAC,EAAAC,cAAA,kBACAF,EAAAC,EAAAC,cAAA,kBACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,uBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,gBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,+DACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,kBACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,qBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,mBAKNF,EAAAC,EAAAC,cAAA,sDAAUF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,wCAAwCC,OAAO,SAASC,IAAI,uBAApE,wCAAV,sEAEAP,EAAAC,EAAAC,cAAA,2DACAF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,wEACAF,EAAAC,EAAAC,cAAA,wJACAF,EAAAC,EAAAC,cAAA,mJAEFF,EAAAC,EAAAC,cAAA,+XAIAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,4zBAiBAT,EAAAC,EAAAC,cAAA,uGAEAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,mBAEAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SACxBX,EAAAC,EAAAC,cAAA,SAAOO,UAAU,aACfT,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,mBACAF,EAAAC,EAAAC,cAAA,mBACAF,EAAAC,EAAAC,cAAA,kBACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,qBACAF,EAAAC,EAAAC,cAAA,uBAGJF,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,gBACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,gBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,gBACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,qBACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,mBAMNF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,kBAEAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SACxBX,EAAAC,EAAAC,cAAA,SAAOO,UAAU,aACfT,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAAA,wBAGJF,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,gBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,mBAMNF,EAAAC,EAAAC,cAAA,qHACAF,EAAAC,EAAAC,cAAA,8YAGAF,EAAAC,EAAAC,cAAA,8BACKF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,yCAAyCC,OAAO,SAASC,IAAI,uBAArE,yCADL,uCACgIP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,wDAAwDC,OAAO,SAASC,IAAI,uBAApF,kBADhI,4HAIAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,6lBAOAT,EAAAC,EAAAC,cAAA,sGAAkBF,EAAAC,EAAAC,cAAA,gDAAlB,8FACAF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,yKACAF,EAAAC,EAAAC,cAAA,+BAAOF,EAAAC,EAAAC,cAAA,oCAAP,0BAAmCF,EAAAC,EAAAC,cAAA,6BAAnC,uFACAF,EAAAC,EAAAC,cAAA,mKAA6BF,EAAAC,EAAAC,cAAA,+BAA7B,oGAGFF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAAA,gKAEAF,EAAAC,EAAAC,cAAA,6CAAWF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,mEAAmEC,OAAO,SAASC,IAAI,uBAA/F,SAAX,wFACAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,oGAIAT,EAAAC,EAAAC,cAAA,qMACAF,EAAAC,EAAAC,cAAA,kZACAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,gBAvOA,+JAyOAT,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,oBAzOA,kJA2OAT,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,mCA3OA,2DA8OAT,EAAAC,EAAAC,cAAA,uCAAUF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,uEAAuEC,OAAO,SAASC,IAAI,uBAAnG,aAAV,qHACAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,oHAIAT,EAAAC,EAAAC,cAAA,6IACoCF,EAAAC,EAAAC,cAAA,wQADpC,UAGAF,EAAAC,EAAAC,cAAA,wHACoBF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,wFAAwFC,OAAO,SAASC,IAAI,uBAApH,0DADpB,eACkLP,EAAAC,EAAAC,cAAA,wEADlL,6CACwNF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,uCAAuCC,OAAO,SAASC,IAAI,uBAAnE,wDADxN,2BACqUP,EAAAC,EAAAC,cAAA,sLADrU,6cAIAF,EAAAC,EAAAC,cAAA,6CAAWF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,qEAAqEC,OAAO,SAASC,IAAI,uBAAjG,WAAX,mNACAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,0GAIAT,EAAAC,EAAAC,cAAA,WAEAF,EAAAC,EAAAC,cAAA,+fACAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,+OAKAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SACxBX,EAAAC,EAAAC,cAAA,SAAOO,UAAU,aACfT,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,uBAGJF,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,yBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,uBACAF,EAAAC,EAAAC,cAAA,qBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,kBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,kBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,qBACAF,EAAAC,EAAAC,cAAA,mBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,mBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,kBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,mBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,kBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,kBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,oBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,kBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,qBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,qBACAF,EAAAC,EAAAC,cAAA,kBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,qBACAF,EAAAC,EAAAC,cAAA,iBACAF,EAAAC,EAAAC,cAAA,oBACAF,EAAAC,EAAAC,cAAA,qBAKNF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,qrDAoCAT,EAAAC,EAAAC,cAAA,kLAAkCF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,mEAAkEC,OAAO,SAASC,IAAI,uBAA9F,SAAlC,+QAEAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,oJAKAT,EAAAC,EAAAC,cAAA,0CAAQF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,yEAAwEC,OAAO,SAASC,IAAI,uBAApG,eAAR,+NAEAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,mRAiBAT,EAAAC,EAAAC,cAAA,SAAGF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,wEAAwEC,OAAO,SAASC,IAAI,uBAApG,cAAH,0gBACAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,8LAQAT,EAAAC,EAAAC,cAAA,ySACAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,oDAEAT,EAAAC,EAAAC,cAAA,8KAA8BF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,mEAAmEC,OAAO,SAASC,IAAI,uBAA/F,SAA9B,sQAEAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,qRAiBAT,EAAAC,EAAAC,cAAA,gPAAyCF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,4BAA4BC,OAAO,SAASC,IAAI,uBAAxD,YAAzC,2HAAuJP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,sDAAsDC,OAAO,SAASC,IAAI,uBAAlF,kCAAvJ,UAEAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,yFAIAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SAASX,EAAAC,EAAAC,cAAA,OAAKU,IAAKC,IAAOC,IAAI,WAEtDd,EAAAC,EAAAC,cAAA,6EACAF,EAAAC,EAAAC,cAAA,iMACAF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,qHACAF,EAAAC,EAAAC,cAAA,sDACAF,EAAAC,EAAAC,cAAA,2GAGFF,EAAAC,EAAAC,cAAA,yCAAYF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,2EAA2EC,OAAO,SAASC,IAAI,uBAAvG,WAAZ,4EACAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,qFAEAT,EAAAC,EAAAC,cAAA,yFAAoBF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,uEAAuEC,OAAO,SAASC,IAAI,uBAAnG,OAApB,wIACAP,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,2jBAEEF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,2XAcFT,EAAAC,EAAAC,cAAA,gEACaF,EAAAC,EAAAC,cAAA,oFADb,2NAEEF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,mBACAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SACxBX,EAAAC,EAAAC,cAAA,SAAOO,UAAU,aACfT,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAAA,wBAGJF,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,gBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,oBAMRF,EAAAC,EAAAC,cAAA,qRAC6DF,EAAAC,EAAAC,cAAA,4GAD7D,+WAEEF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,2BAEFT,EAAAC,EAAAC,cAAA,ySAEEF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,kBAGJT,EAAAC,EAAAC,cAAA,maAGAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,yFACAT,EAAAC,EAAAC,cAAA,+BACAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,QAArB,4MAOAT,EAAAC,EAAAC,cAAA,eAASF,EAAAC,EAAAC,cAAA,6DACTF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,QAArB,ghbA6MAT,EAAAC,EAAAC,cAAA,mCACAF,EAAAC,EAAAC,cAAA,kkBAGAF,EAAAC,EAAAC,cAAA,2GAC+CF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,oFAAoFC,OAAO,SAASC,IAAI,uBAAhH,QAD/C,2HACiNP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,0BAA0BC,OAAO,SAASC,IAAI,uBAAtD,cADjN,kVAGAP,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,QAArB,6BAEAT,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,gFAIAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SACxBX,EAAAC,EAAAC,cAAA,SAAOO,UAAU,aACfT,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,WACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,qBACAF,EAAAC,EAAAC,cAAA,kBACAF,EAAAC,EAAAC,cAAA,mBAGJF,EAAAC,EAAAC,cAAA,aACEF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,uBAEFF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,eACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,sBACAF,EAAAC,EAAAC,cAAA,0BAMNF,EAAAC,EAAAC,cAAA,2IACAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,wCACAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SAASX,EAAAC,EAAAC,cAAA,OAAKU,IAAKG,IAAMD,IAAI,eAErDd,EAAAC,EAAAC,cAAA,6PACAF,EAAAC,EAAAC,cAACM,EAAAP,EAAD,CAAWQ,UAAU,UAArB,sCACAT,EAAAC,EAAAC,cAAA,OAAKQ,MAAO,CAACC,UAAW,SAASX,EAAAC,EAAAC,cAAA,OAAKU,IAAKI,IAAKF,IAAI,mBAEpDd,EAAAC,EAAAC,cAAA,wKAC4BF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,mCAAmCC,OAAO,SAASC,IAAI,uBAA/D,8EAD5B,0DAIAP,EAAAC,EAAAC,cAAA,sCACAF,EAAAC,EAAAC,cAAA,0WACAF,EAAAC,EAAAC,cAAA,+WAAmFF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,mCAAmCC,OAAO,SAASC,IAAI,uBAA/D,gBAAnF,SAAyLP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,gGAAgGC,OAAO,SAASC,IAAI,uBAA5H,yBAAzL,SAAqWP,EAAAC,EAAAC,cAAA,KAAGG,KAAK,8FAA8FC,OAAO,SAASC,IAAI,uBAA1H,gBAArW,wLACAP,EAAAC,EAAAC,cAAA,2MACAF,EAAAC,EAAAC,cAAA,UACEF,EAAAC,EAAAC,cAAA,8SACAF,EAAAC,EAAAC,cAAA,kOAEFF,EAAAC,EAAAC,cAAA,wnBAGAF,EAAAC,EAAAC,cAAA,4bAC0EF,EAAAC,EAAAC,cAAA,KAAGG,KAAK,kCAAkCC,OAAO,SAASC,IAAI,uBAA9D,WAD1E,kEAvxB0CU,IAAMC","file":"static/js/4.62a187af.chunk.js","sourcesContent":["module.exports = __webpack_public_path__ + \"static/media/model.5355f0bf.png\";","module.exports = __webpack_public_path__ + \"static/media/loss_x_val_loss.278fc272.png\";","module.exports = __webpack_public_path__ + \"static/media/acc_x_val_acc.5d13768a.png\";","import React from \"react\";\nimport Highlight from \"react-highlight\";\nimport \"../Blog.css\";\nimport model from \"./model.png\";\nimport loss from \"./loss_x_val_loss.png\";\nimport acc from \"./acc_x_val_acc.png\";\n\nexport default class Post20190119 extends React.Component {\n  componendDidMount() {\n    document.title = \"Kznovo - \" + this.props.title;\n  }\n\n  render() {\n    return(\n<>\n<h2>{this.props.title}</h2>\n<small>{this.props.date}</small>\n<br/>\n<p>\n  Entity Embeddingsという深層学習の手法があります。深層学習がよく使われる画像分析や音声分析などのデータとは違う、<strong>カテゴリ変数</strong>や<strong>順序変数</strong>の特徴量を学習する時に使います。\n</p>\n<p>\n  Entity Embeddingsが広く知られるようになったきっかけは、Kaggleの<a href=\"https://www.kaggle.com/c/rossmann-store-sales\" target=\"_blank\" rel=\"noopener noreferrer\">Rossmann Store Sales</a>コンペでした。１位と２位のチームがドメイン知識をフル活用したアプローチをしたのに対し、この手法を活用したチームはドメイン知識の無い中なんと３位に入賞しました。コンペの説明と、使われた手法については、３位のNeokami Incの<a href=\"http://blog.kaggle.com/2016/01/22/rossmann-store-sales-winners-interview-3rd-place-cheng-gui/\" target=\"_blank\" rel=\"noopener noreferrer\">インタビュー記事</a>、<a href=\"https://github.com/entron/entity-embedding-rossmann\" target=\"_blank\" rel=\"noopener noreferrer\">使われたソースコード</a>、コンペ後に発表した手法に関する<a href=\"https://arxiv.org/abs/1604.06737\" target=\"_blank\" rel=\"noopener noreferrer\">論文</a>などで学ぶことができます。\n</p>\n\n<p>\n  <a href=\"https://www.kaggle.com/c/titanic\" target=\"_blank\" rel=\"noopener noreferrer\">タイタニック号生存者予測コンペ</a>のサンプルデータに対し、このEntity Embeddingsを実装するにはどうすれば良いのでしょうか。\n</p>\n\n<h3>0. 環境構築</h3>\n<p>\n  環境構築に私は<a href=\"https://pipenv.readthedocs.io/en/latest/\" target=\"_blank\" rel=\"noopener noreferrer\">Pipenv</a>を使っています。venv、conda、pip+virtualenv、pyenvなど、他のツールを使っている方はそれを使ってください。今回はPipenvを使った書き方のみ紹介します。\n</p>\n<p>\n  <a href=\"https://www.python.org/downloads/release/python-367/\" target=\"_blank\" rel=\"noopener noreferrer\">Python3.6</a>を使います。今回使う<a href=\"https://www.tensorflow.org/versions/r1.12/api_docs/python/tf\" target=\"_blank\" rel=\"noopener noreferrer\">Tensorflow (v1.12)</a>がPython3.7以降と互換性が無いため（執筆時）です。また、データ読み込み、前処理、表示などのために<a href=\"https://pandas.pydata.org/\" target=\"_blank\" rel=\"noopener noreferrer\">pandas</a>もインストールします。\n</p>\n<Highlight className=\"bash\">\n{`pipenv --python 3.6\npipenv install tensorflow pandas`}</Highlight>\n\n<p>\n  <a href=\"https://jupyter.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Jupyter Notebook</a> で実行するときは、インストールして起動しましょう。\n</p>\n<Highlight className=\"bash\">\n{`pipenv install jupyter\npipenv run jupyter notebook`}</Highlight>\n\n\n<h3>1. データを読み込む</h3>\n<p>\n  使うデータはタイタニックコンペのデータです。<a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\" rel=\"noopener noreferrer\">リンク</a>からデータをダウンロードして、解凍しておきます。\n</p>\n<Highlight className=\"python\">\n{`import pandas as pd\n\ntrain = pd.read_csv(\"./train.csv\")\ntest = pd.read_csv(\"./test.csv\")\n\ntrain.head(2)`}</Highlight>\n<div style={{overflowX: \"auto\"}}>\n<table className=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<p>各項目の説明は<a href=\"https://www.kaggle.com/c/titanic/data\" target=\"_blank\" rel=\"noopener noreferrer\">コンペページ</a>で説明がされています。</p>\n\n<h3>2. データの前処理</h3>\n<ul>\n  <li>Null値に適当な値を代入</li>\n  <li>文字列データは、カテゴリ変数（数字）に変換する</li>\n  <li>データから新しい特徴を見つけ、データを増やす</li>\n</ul>\n<p>\n  今回は比較的簡単な処理のみをしています。詳細については割愛しますが、python + pandas ではこんなに少ない行数で様々な処理を実行できます。\n</p>\n\n<Highlight className=\"python\">\n{`data = pd.concat([train.drop(columns=\"Survived\"), test]).reset_index(drop=True)\n\ndata[\"Pclass\"] -= 1\ndata[\"FamSize\"] = data[\"SibSp\"] + data[\"Parch\"]\ndata[\"IsAlone\"] = data[\"FamSize\"] == 0\ndata[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0], inplace=True)\ndata[\"Age\"].fillna(data[\"Age\"].median(), inplace=True)\ndata[\"Fare\"].fillna(data[\"Fare\"].median(), inplace=True)\ndata.drop(columns=[\"Cabin\", \"Ticket\", \"Name\"], inplace=True)\nfor c in [\"Sex\", \"Embarked\"]: data[c] = data[c].astype(\"category\").cat.codes\nfor c in [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"FamSize\", \"IsAlone\"]: data[c] = data[c].astype(int)\n\ntrain_X = data.join(train[\"Survived\"], how=\"inner\").drop(columns=[\"PassengerId\", \"Survived\"])\ntrain_Y = train[[\"Survived\"]]\ntest_X = data.loc[data[\"PassengerId\"].isin(test[\"PassengerId\"])]`}</Highlight>\n\n<p>全て数字のデータができました。</p>\n\n<Highlight className=\"python\">train_X.head(2)</Highlight>\n\n<div style={{overflowX: \"auto\"}}>\n<table className=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Embarked</th>\n      <th>FamSize</th>\n      <th>IsAlone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>1</td>\n      <td>22</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>38</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n<Highlight className=\"python\">test_Y.head(2)</Highlight>\n\n<div style={{overflowX: \"auto\"}}>\n<table className=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n<h3>3. Entity Embeddingを用いた深層学習モデルを作る</h3>\n<p>\n  Embeddingを作る対象は、「カテゴリ変数」と「順序変数」です。前処理後のタイタニックデータの中では、「Fare」をのぞいて全てが当てはまります。\n</p>\n<p>\n  今回は<a href=\"https://www.tensorflow.org/guide/keras\" target=\"_blank\" rel=\"noopener noreferrer\">Tensorflowの中のKeras API</a>を使います。<a href=\"https://www.tensorflow.org/guide/keras#functional_api\" target=\"_blank\" rel=\"noopener noreferrer\">Functional API</a>という記法を使って以下のように書きます。\n</p>\n\n<Highlight className=\"python\">\n{`from tensorflow.keras.layers import Input, Embedding, Flatten\n\nx = Input(shape=(1,))                         # 入力層。\nx = Embedding(\"カテゴリ数\", \"埋め込み次元数\")(x)  # 入力されるカテゴリ数と埋め込み次元数をそれぞれ指定する。\nx = Flatten()(x)                              # 次ステップで組み合わせるときのために形を整える。`}</Highlight>\n\n<p>上記で１ブロックとなり、これを<strong>各項目ごと</strong>に作ります。残りのステップは、</p>\n<ol>\n  <li>埋め込みブロックの出力を全て組み合わせて（Concatenate層）、</li>\n  <li>何層か<strong>全結合</strong>（Dense層）<strong>+ReLU層</strong>（Dense層の引数で指定）を足し、</li>\n  <li>二値分類（生存したか、していないか）のため、最後を<strong>Sigmoid層</strong>（Dense層の引数で指定）にして完成。</li>\n</ol>\n\n<hr/>\n<p>「Pclass」のEmbeddingブロックを作る過程で詳しく説明します。</p>\n\n<h4>イ. まずは、<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Input\" target=\"_blank\" rel=\"noopener noreferrer\">Input</a>層を使って入力層を作ります。</h4>\n<Highlight className=\"python\">\n{`from tensorflow.keras.layers import Input\ninput_Pclass = Input(shape=(1,), name=\"input_Pclass\")`}\n</Highlight>\n<p>name引数を使って、層に名前をつけておきます（後ほど使用します）。</p>\n<p>尚、入力する形（shape引数）が (1,) なのは、データ１単位あたり必ず１つの数字のみが入力されるためです。つまり、タイタニック乗客一人に対して</p>\n<Highlight className=\"python\">Pclass = [1]</Highlight>\nこのようなデータの単位を持っているからです。例えば、\n<Highlight className=\"python\">data = [0, 1, 3]</Highlight>\n上記が単位あたりのデータの場合、入力する形は (3,)。\n<Highlight className=\"python\">data = [[0, 2], [3, 1], [4, 0]]</Highlight>\nこの場合は (3, 2) です。\n\n<h4>ロ. 次に、<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\" target=\"_blank\" rel=\"noopener noreferrer\">Embedding</a>層を使い、Embeddingベクトルを割り当てます。  </h4>\n<Highlight className=\"python\">\n{`from tensorflow.keras.layers import Embedding\nembed_Pclass = Embedding(3, 2, name=\"embed_Pclass\")(input_Pclass)`}\n</Highlight>\n<p>\n  Pclassのカテゴリ数は、「0 か 1 か 2」 の「3つ」。尚、<strong>埋め込み次元数はハイパーパラメーターのため、正解が無く、試行錯誤する必要があります</strong>。\n</p>\n<p>\n  埋め込み次元数で推奨される大きさは、<a href=\"https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html\" target=\"_blank\" rel=\"noopener noreferrer\">Googleのチュートリアル</a>では<strong>「カテゴリー数**0.25」</strong>、大元の論文（<a href=\"https://arxiv.org/pdf/1604.06737.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">セクションV. 5段落目</a>）では、<strong>「カテゴリ数〜カテゴリ数-1の間の数字で試しながら決める」</strong>などと書かれており、まだベストプラクティスが決まっていない状態です。今回は、3つのPclassのカテゴリ変数に対して、大きさ「２」のベクトルをそれぞれ割り当てます。\n</p>\n\n<h4>ハ. 最後に、<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten\" target=\"_blank\" rel=\"noopener noreferrer\">Flatten</a>層を使って、後ほど他のEmbeddingブロックと結合するときのために形を整えます。</h4>\n<Highlight className=\"python\">\n{`from tensorflow.keras.layers import Flatten\nflatt_Pclass = Flatten(name=\"flatt_Pclass\")(embed_Pclass)`}</Highlight>\n\n<hr/>\n\n<p>以上で１ブロック完成です。以下が全ての項目でこの操作をしたコードです。まず、各項目の最小値、最大値、ユニークな値の数を調べ、それを元に埋め込み次元数を決めていきます。</p>\n<Highlight className=\"python\">\n{`# データの各項目の「最小値」、「最大値」、「ユニークな値数」を表示\ndata.agg([\"min\", \"max\", \"nunique\"]).T\n`}</Highlight>\n\n<div style={{overflowX: \"auto\"}}>\n<table className=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>min</th>\n      <th>max</th>\n      <th>nunique</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>PassengerId</th>\n      <td>1.0</td>\n      <td>1309.0000</td>\n      <td>1309.0</td>\n    </tr>\n    <tr>\n      <th>Pclass</th>\n      <td>0.0</td>\n      <td>2.0000</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>Sex</th>\n      <td>0.0</td>\n      <td>1.0000</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>Age</th>\n      <td>0.0</td>\n      <td>80.0000</td>\n      <td>73.0</td>\n    </tr>\n    <tr>\n      <th>SibSp</th>\n      <td>0.0</td>\n      <td>8.0000</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>Parch</th>\n      <td>0.0</td>\n      <td>9.0000</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>Fare</th>\n      <td>0.0</td>\n      <td>512.3292</td>\n      <td>281.0</td>\n    </tr>\n    <tr>\n      <th>Embarked</th>\n      <td>0.0</td>\n      <td>2.0000</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>FamSize</th>\n      <td>0.0</td>\n      <td>10.0000</td>\n      <td>9.0</td>\n    </tr>\n    <tr>\n      <th>IsAlone</th>\n      <td>0.0</td>\n      <td>1.0000</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n<Highlight className=\"python\">\n{`# 上記のデータを元に各カテゴリ数や埋め込み次元を決めてブロックを作っていく\nfrom tensorflow.keras.layers import Input, Embedding, Flatten\n\ninput_Pclass = Input(shape=(1,), name=\"input_Pclass\")\nembed_Pclass = Embedding(3, 2, name=\"embed_Pclass\")(input_Pclass)\nflatt_Pclass = Flatten(name=\"flatt_Pclass\")(embed_Pclass)\n\ninput_Sex = Input(shape=(1,), name=\"input_Sex\")\nembed_Sex = Embedding(2, 4, name=\"embed_Sex\")(input_Sex)\nflatt_Sex = Flatten(name=\"flatt_Sex\")(embed_Sex)\n\ninput_Age = Input(shape=(1,), name=\"input_Age\")\nembed_Age = Embedding(81, 15, name=\"embed_Age\")(input_Age)\nflatt_Age = Flatten(name=\"flatt_Age\")(embed_Age)\n\ninput_SibSp = Input(shape=(1,), name=\"input_SibSp\")\nembed_SibSp = Embedding(9, 7, name=\"embed_SibSp\")(input_SibSp)\nflatt_SibSp = Flatten(name=\"flatt_SibSp\")(embed_SibSp)\n\ninput_Parch = Input(shape=(1,), name=\"input_Parch\")\nembed_Parch = Embedding(10, 8, name=\"embed_Parch\")(input_Parch)\nflatt_Parch = Flatten(name=\"flatt_Parch\")(embed_Parch)\n\ninput_Embarked = Input(shape=(1,), name=\"input_Embarked\")\nembed_Embarked = Embedding(4, 3, name=\"embed_Embarked\")(input_Pclass)\nflatt_Embarked = Flatten(name=\"flatt_Embarked\")(embed_Pclass)\n\ninput_FamSize = Input(shape=(1,), name=\"input_FamSize\")\nembed_FamSize = Embedding(11, 6, name=\"embed_FamSize\")(input_FamSize)\nflatt_FamSize = Flatten(name=\"flatt_FamSize\")(embed_FamSize)\n\ninput_IsAlone = Input(shape=(1,), name=\"input_IsAlone\")\nembed_IsAlone = Embedding(2, 4, name=\"embed_IsAlone\")(input_IsAlone)\nflatt_IsAlone = Flatten(name=\"flatt_IsAlone\")(embed_IsAlone)`}</Highlight>\n\n<p>Fareだけは連続値のため、別の処理をします。今回は全結合層（<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\"target=\"_blank\" rel=\"noopener noreferrer\">Dense</a>）のみです。全結合層は形が他のEmbeddingブロックの出力層と一緒になるため整える必要はありません。</p>\n\n<Highlight className=\"python\">\n{`from tensorflow.keras.layers import Dense\ninput_Fare = Input(shape=(1,), name=\"input_Fare\")\ndense_Fare = Dense(30, name=\"fc_Fare\")(input_Fare)`}</Highlight>\n\n<p>全ての層を<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate\"target=\"_blank\" rel=\"noopener noreferrer\">Concatenate</a>層を使って一つの層にします。「flatt_」で始まる層と、FareのDense層を全てまとめます。</p>\n\n<Highlight className=\"python\">\n{`from tensorflow.keras.layers import Concatenate\n\nconcat_layers= [\n    flatt_Pclass,\n    flatt_Sex,\n    flatt_Age,\n    flatt_SibSp,\n    flatt_Parch,\n    flatt_Embarked,\n    flatt_FamSize,\n    flatt_IsAlone,\n    dense_Fare,\n]\n\nx = Concatenate()(concat_layers)`}</Highlight>\n\n<p><a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation\" target=\"_blank\" rel=\"noopener noreferrer\">Activation</a>層を使って活性化関数（ReLU）を適用してから、全結合＋ReLUのブロックを３つ繋げてみます。全結合層のニューロンの数（第一引数）は適当に選び、トレーニングをしながらチューニングします。</p>\n<Highlight className=\"python\">\n{`from tensorflow.keras.layers import Activation\n\nx = Activation(\"relu\")(x)\nx = Dense(96, activation=\"relu\")(x)\nx = Dense(48, activation=\"relu\")(x)\nx = Dense(24, activation=\"relu\")(x)`}</Highlight>\n\n<p>最終出力は、二値分類のため、ニューロンを1つだけ持った全結合層にSigmoid層を組み合わせたブロックです。</p>\n<Highlight className=\"python\">output_layer = Dense(1, activation=\"sigmoid\")(x)</Highlight>\n\n<p>ここまでで作った層を全てまとめて、モデルを定義します。<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/models/Model\" target=\"_blank\" rel=\"noopener noreferrer\">Model</a>クラスに入力層と出力層を渡すと自動で中身の層を繋げてくれます。以下のように入力します。</p>\n\n<Highlight className=\"python\">\n{`from tensorflow.keras.models import Model\n\ninput_layers = [\n    input_Pclass,\n    input_Sex,\n    input_Age,\n    input_SibSp,\n    input_Parch,\n    input_Embarked,\n    input_FamSize,\n    input_IsAlone,\n    input_Fare\n]\n\nmodel = Model(input_layers, output_layer)`}</Highlight>\n\n<p>これを図にすると以下のようになります。（このコードを実行するには、実行環境に<a href=\"https://www.graphviz.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Graphviz</a>がインストールされている必要があります。<a href=\"https://github.com/XifengGuo/CapsNet-Keras/issues/7\" target=\"_blank\" rel=\"noopener noreferrer\">参考リンク</a>）</p>\n\n<Highlight className=\"python\">\n{`from tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model.png')`}</Highlight>\n\n<div style={{overflowX: \"auto\"}}><img src={model} alt=\"model\"/></div>\n\n<h3>4. 作成したモデルで学習</h3>\n<p>学習の際には、いくつかの変数を手動で設定する必要があります。</p>\n<ul>\n  <li>損失関数は二値分類のため「binary-crossentropy」</li>\n  <li>勾配法は「Adam」</li>\n  <li>モデルのパフォーマンスは「accuracy」</li>\n</ul>\n\n<p>Modelクラスの<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#compile\" target=\"_blank\" rel=\"noopener noreferrer\">compile</a>関数で全て設定できます。</p>\n<Highlight className=\"python\">model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])</Highlight>\n\n<p>トレーニングにはModelクラスの<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit\" target=\"_blank\" rel=\"noopener noreferrer\">fit</a>関数を使います。今回引数に渡す変数について：</p>\n<ol>\n  <li>\n    x: 入力データ。pythonのdictionary型を使うことで、各Input層でつけていた名前を使い項目ごとのデータを指定することができます。尚、入力の形を各項目で(batch_size, 1)としたいため、PandasのDataFrame項目の選択の仕方を工夫してます。\n    <Highlight className=\"python\">\n{`X = {\n    \"input_Pclass\": train_X[[\"Pclass\"]],\n    \"input_Sex\": train_X[[\"Sex\"]],\n    \"input_Age\": train_X[[\"Age\"]],\n    \"input_SibSp\": train_X[[\"SibSp\"]],\n    \"input_Parch\": train_X[[\"Parch\"]],\n    \"input_Embarked\": train_X[[\"Embarked\"]],\n    \"input_FamSize\": train_X[[\"FamSize\"]],\n    \"input_IsAlone\": train_X[[\"IsAlone\"]],\n    \"input_Fare\": train_X[[\"Fare\"]],\n}`}\n    </Highlight>\n  </li>\n  <li>\n    y: 目的変数。つまり<strong>生存したかしていないか</strong>のデータ項目。これはDataFrame型で、入力の形が(batch_size, 1)となるためそのまま使います。\n    <Highlight className=\"python\">train_Y.head(2)</Highlight>\n    <div style={{overflowX: \"auto\"}}>\n    <table className=\"dataframe\">\n      <thead>\n        <tr>\n          <th></th>\n          <th>Survived</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr>\n          <th>0</th>\n          <td>0</td>\n        </tr>\n        <tr>\n          <th>1</th>\n          <td>1</td>\n        </tr>\n      </tbody>\n    </table>\n    </div>\n  </li>\n  <li>\n    validation_split: 機械学習では、モデルが「過学習」を起こしていないかモニタリングするために、データを<strong>「トレーニング用」と「検証用」</strong>に分けます。fit関数では、ここに分割する割合を入れると自動で分割してくれます。2割のデータをテスト用に分割して分けておく場合：\n    <Highlight className=\"python\">validation_split = 0.2</Highlight>\n  </li>\n  <li>\n    epochs: 全データを一周学習することを「1 epoch（エポック）学習する」と言います。何epoch学習するか入力します。\n    <Highlight className=\"python\">epochs = 100</Highlight>\n  </li>\n</ol>\n<p>\n  1~4の変数をfit関数に引数として渡します。その他はデフォルトを使います。尚、fit関数からは「History」オブジェクトというものが返ります（詳細は後述）。\n</p>\n<Highlight className=\"python\">history = model.fit(x=x, y=train_Y, validation_split=validation_split, epochs=epochs)</Highlight>\n<p>出力：</p>\n<Highlight className=\"bash\">\n{`Train on 712 samples, validate on 179 samples\nEpoch 1/100\n712/712 [==============================] - 1s 2ms/step - loss: 0.9792 - acc: 0.5281 - val_loss: 0.6805 - val_acc: 0.3855\nEpoch 2/100\n.....`}\n</Highlight>\n<details><summary>学習経過の詳細</summary>\n<Highlight className=\"bash\">\n{`Train on 712 samples, validate on 179 samples\nEpoch 1/100\n712/712 [==============================] - 1s 2ms/step - loss: 0.9792 - acc: 0.5281 - val_loss: 0.6805 - val_acc: 0.3855\nEpoch 2/100\n712/712 [==============================] - 0s 146us/step - loss: 0.6960 - acc: 0.5590 - val_loss: 0.6561 - val_acc: 0.6872\nEpoch 3/100\n712/712 [==============================] - 0s 150us/step - loss: 0.6851 - acc: 0.6475 - val_loss: 0.6511 - val_acc: 0.6983\nEpoch 4/100\n712/712 [==============================] - 0s 138us/step - loss: 0.6722 - acc: 0.6306 - val_loss: 0.6286 - val_acc: 0.7039\nEpoch 5/100\n712/712 [==============================] - 0s 145us/step - loss: 0.6510 - acc: 0.6433 - val_loss: 0.5984 - val_acc: 0.7151\nEpoch 6/100\n712/712 [==============================] - 0s 156us/step - loss: 0.6345 - acc: 0.6685 - val_loss: 0.6018 - val_acc: 0.6536\nEpoch 7/100\n712/712 [==============================] - 0s 140us/step - loss: 0.6019 - acc: 0.6685 - val_loss: 0.5386 - val_acc: 0.7374\nEpoch 8/100\n712/712 [==============================] - 0s 138us/step - loss: 0.6225 - acc: 0.6812 - val_loss: 0.4991 - val_acc: 0.7486\nEpoch 9/100\n712/712 [==============================] - 0s 137us/step - loss: 0.5585 - acc: 0.7135 - val_loss: 0.4806 - val_acc: 0.7765\nEpoch 10/100\n712/712 [==============================] - 0s 126us/step - loss: 0.5909 - acc: 0.7303 - val_loss: 0.4898 - val_acc: 0.7933\nEpoch 11/100\n712/712 [==============================] - 0s 124us/step - loss: 0.5234 - acc: 0.7542 - val_loss: 0.4483 - val_acc: 0.7877\nEpoch 12/100\n712/712 [==============================] - 0s 135us/step - loss: 0.5222 - acc: 0.7697 - val_loss: 0.4479 - val_acc: 0.7933\nEpoch 13/100\n712/712 [==============================] - 0s 133us/step - loss: 0.4956 - acc: 0.7893 - val_loss: 0.4542 - val_acc: 0.8268\nEpoch 14/100\n712/712 [==============================] - 0s 134us/step - loss: 0.4756 - acc: 0.7949 - val_loss: 0.4332 - val_acc: 0.8045\nEpoch 15/100\n712/712 [==============================] - 0s 133us/step - loss: 0.4804 - acc: 0.8062 - val_loss: 0.4206 - val_acc: 0.8045\nEpoch 16/100\n712/712 [==============================] - 0s 133us/step - loss: 0.4732 - acc: 0.7935 - val_loss: 0.4278 - val_acc: 0.7933\nEpoch 17/100\n712/712 [==============================] - 0s 150us/step - loss: 0.5102 - acc: 0.7837 - val_loss: 0.4183 - val_acc: 0.7933\nEpoch 18/100\n712/712 [==============================] - 0s 165us/step - loss: 0.5006 - acc: 0.7907 - val_loss: 0.4251 - val_acc: 0.8268\nEpoch 19/100\n712/712 [==============================] - 0s 156us/step - loss: 0.4516 - acc: 0.8034 - val_loss: 0.4659 - val_acc: 0.8045\nEpoch 20/100\n712/712 [==============================] - 0s 158us/step - loss: 0.4978 - acc: 0.8034 - val_loss: 0.4105 - val_acc: 0.8268\nEpoch 21/100\n712/712 [==============================] - 0s 154us/step - loss: 0.4582 - acc: 0.8076 - val_loss: 0.4049 - val_acc: 0.8156\nEpoch 22/100\n712/712 [==============================] - 0s 155us/step - loss: 0.4605 - acc: 0.8090 - val_loss: 0.4055 - val_acc: 0.8156\nEpoch 23/100\n712/712 [==============================] - 0s 148us/step - loss: 0.4480 - acc: 0.8118 - val_loss: 0.4121 - val_acc: 0.8324\nEpoch 24/100\n712/712 [==============================] - 0s 166us/step - loss: 0.4371 - acc: 0.8216 - val_loss: 0.4107 - val_acc: 0.8268\nEpoch 25/100\n712/712 [==============================] - 0s 157us/step - loss: 0.4236 - acc: 0.8160 - val_loss: 0.4387 - val_acc: 0.8101\nEpoch 26/100\n712/712 [==============================] - 0s 149us/step - loss: 0.4274 - acc: 0.8230 - val_loss: 0.4568 - val_acc: 0.8045\nEpoch 27/100\n712/712 [==============================] - 0s 136us/step - loss: 0.4208 - acc: 0.8230 - val_loss: 0.4769 - val_acc: 0.7989\nEpoch 28/100\n712/712 [==============================] - 0s 135us/step - loss: 0.4291 - acc: 0.8216 - val_loss: 0.3998 - val_acc: 0.8436\nEpoch 29/100\n712/712 [==============================] - 0s 141us/step - loss: 0.4105 - acc: 0.8258 - val_loss: 0.3965 - val_acc: 0.8380\nEpoch 30/100\n712/712 [==============================] - 0s 136us/step - loss: 0.4240 - acc: 0.8146 - val_loss: 0.3976 - val_acc: 0.8045\nEpoch 31/100\n712/712 [==============================] - 0s 138us/step - loss: 0.4214 - acc: 0.8202 - val_loss: 0.4079 - val_acc: 0.8268\nEpoch 32/100\n712/712 [==============================] - 0s 159us/step - loss: 0.4475 - acc: 0.8160 - val_loss: 0.3961 - val_acc: 0.8380\nEpoch 33/100\n712/712 [==============================] - 0s 181us/step - loss: 0.4724 - acc: 0.8188 - val_loss: 0.4297 - val_acc: 0.8156\nEpoch 34/100\n712/712 [==============================] - 0s 157us/step - loss: 0.4273 - acc: 0.8118 - val_loss: 0.4001 - val_acc: 0.8380\nEpoch 35/100\n712/712 [==============================] - 0s 124us/step - loss: 0.4035 - acc: 0.8301 - val_loss: 0.3980 - val_acc: 0.8045\nEpoch 36/100\n712/712 [==============================] - 0s 119us/step - loss: 0.4583 - acc: 0.8132 - val_loss: 0.4186 - val_acc: 0.8268\nEpoch 37/100\n712/712 [==============================] - 0s 117us/step - loss: 0.4608 - acc: 0.8188 - val_loss: 0.4096 - val_acc: 0.8045\nEpoch 38/100\n712/712 [==============================] - 0s 115us/step - loss: 0.4323 - acc: 0.8216 - val_loss: 0.3896 - val_acc: 0.8268\nEpoch 39/100\n712/712 [==============================] - 0s 120us/step - loss: 0.4008 - acc: 0.8287 - val_loss: 0.4032 - val_acc: 0.8324\nEpoch 40/100\n712/712 [==============================] - 0s 128us/step - loss: 0.4063 - acc: 0.8329 - val_loss: 0.3927 - val_acc: 0.8268\nEpoch 41/100\n712/712 [==============================] - 0s 122us/step - loss: 0.3962 - acc: 0.8301 - val_loss: 0.3884 - val_acc: 0.8324\nEpoch 42/100\n712/712 [==============================] - 0s 124us/step - loss: 0.3980 - acc: 0.8244 - val_loss: 0.3898 - val_acc: 0.8380\nEpoch 43/100\n712/712 [==============================] - 0s 123us/step - loss: 0.4049 - acc: 0.8371 - val_loss: 0.3938 - val_acc: 0.8268\nEpoch 44/100\n712/712 [==============================] - 0s 118us/step - loss: 0.4251 - acc: 0.8258 - val_loss: 0.4529 - val_acc: 0.8268\nEpoch 45/100\n712/712 [==============================] - 0s 136us/step - loss: 0.3988 - acc: 0.8385 - val_loss: 0.3931 - val_acc: 0.8268\nEpoch 46/100\n712/712 [==============================] - 0s 152us/step - loss: 0.4082 - acc: 0.8244 - val_loss: 0.4085 - val_acc: 0.8045\nEpoch 47/100\n712/712 [==============================] - 0s 141us/step - loss: 0.3942 - acc: 0.8301 - val_loss: 0.3887 - val_acc: 0.8436\nEpoch 48/100\n712/712 [==============================] - 0s 138us/step - loss: 0.3921 - acc: 0.8399 - val_loss: 0.3959 - val_acc: 0.8324\nEpoch 49/100\n712/712 [==============================] - 0s 147us/step - loss: 0.4025 - acc: 0.8287 - val_loss: 0.4297 - val_acc: 0.8380\nEpoch 50/100\n712/712 [==============================] - 0s 151us/step - loss: 0.4080 - acc: 0.8287 - val_loss: 0.3959 - val_acc: 0.8380\nEpoch 51/100\n712/712 [==============================] - 0s 153us/step - loss: 0.4170 - acc: 0.8244 - val_loss: 0.4442 - val_acc: 0.8268\nEpoch 52/100\n712/712 [==============================] - 0s 147us/step - loss: 0.4064 - acc: 0.8301 - val_loss: 0.4091 - val_acc: 0.8436\nEpoch 53/100\n712/712 [==============================] - 0s 155us/step - loss: 0.3900 - acc: 0.8343 - val_loss: 0.3924 - val_acc: 0.8436\nEpoch 54/100\n712/712 [==============================] - 0s 155us/step - loss: 0.3940 - acc: 0.8371 - val_loss: 0.4157 - val_acc: 0.8436\nEpoch 55/100\n712/712 [==============================] - 0s 135us/step - loss: 0.3775 - acc: 0.8385 - val_loss: 0.4024 - val_acc: 0.8436\nEpoch 56/100\n712/712 [==============================] - 0s 139us/step - loss: 0.3766 - acc: 0.8371 - val_loss: 0.3946 - val_acc: 0.8436\nEpoch 57/100\n712/712 [==============================] - 0s 140us/step - loss: 0.3749 - acc: 0.8357 - val_loss: 0.3940 - val_acc: 0.8492\nEpoch 58/100\n712/712 [==============================] - 0s 158us/step - loss: 0.3691 - acc: 0.8371 - val_loss: 0.4350 - val_acc: 0.8324\nEpoch 59/100\n712/712 [==============================] - 0s 152us/step - loss: 0.3753 - acc: 0.8371 - val_loss: 0.4079 - val_acc: 0.8380\nEpoch 60/100\n712/712 [==============================] - 0s 148us/step - loss: 0.3667 - acc: 0.8371 - val_loss: 0.4148 - val_acc: 0.8436\nEpoch 61/100\n712/712 [==============================] - 0s 151us/step - loss: 0.3787 - acc: 0.8427 - val_loss: 0.4516 - val_acc: 0.8380\nEpoch 62/100\n712/712 [==============================] - 0s 169us/step - loss: 0.3747 - acc: 0.8287 - val_loss: 0.4135 - val_acc: 0.8436\nEpoch 63/100\n712/712 [==============================] - 0s 145us/step - loss: 0.3641 - acc: 0.8427 - val_loss: 0.4260 - val_acc: 0.8380\nEpoch 64/100\n712/712 [==============================] - 0s 140us/step - loss: 0.3675 - acc: 0.8427 - val_loss: 0.4239 - val_acc: 0.8436\nEpoch 65/100\n712/712 [==============================] - 0s 178us/step - loss: 0.3594 - acc: 0.8385 - val_loss: 0.4164 - val_acc: 0.8436\nEpoch 66/100\n712/712 [==============================] - 0s 181us/step - loss: 0.3599 - acc: 0.8385 - val_loss: 0.4291 - val_acc: 0.8380\nEpoch 67/100\n712/712 [==============================] - 0s 136us/step - loss: 0.3536 - acc: 0.8483 - val_loss: 0.4112 - val_acc: 0.8492\nEpoch 68/100\n712/712 [==============================] - 0s 137us/step - loss: 0.4036 - acc: 0.8301 - val_loss: 0.4338 - val_acc: 0.8156\nEpoch 69/100\n712/712 [==============================] - 0s 141us/step - loss: 0.3842 - acc: 0.8287 - val_loss: 0.4204 - val_acc: 0.8547\nEpoch 70/100\n712/712 [==============================] - 0s 144us/step - loss: 0.3648 - acc: 0.8427 - val_loss: 0.4215 - val_acc: 0.8380\nEpoch 71/100\n712/712 [==============================] - 0s 141us/step - loss: 0.3541 - acc: 0.8455 - val_loss: 0.4551 - val_acc: 0.8380\nEpoch 72/100\n712/712 [==============================] - 0s 178us/step - loss: 0.3618 - acc: 0.8455 - val_loss: 0.4187 - val_acc: 0.8492\nEpoch 73/100\n712/712 [==============================] - 0s 145us/step - loss: 0.3636 - acc: 0.8357 - val_loss: 0.4310 - val_acc: 0.8380\nEpoch 74/100\n712/712 [==============================] - 0s 130us/step - loss: 0.3472 - acc: 0.8427 - val_loss: 0.4603 - val_acc: 0.8324\nEpoch 75/100\n712/712 [==============================] - 0s 125us/step - loss: 0.3511 - acc: 0.8413 - val_loss: 0.4390 - val_acc: 0.8436\nEpoch 76/100\n712/712 [==============================] - 0s 133us/step - loss: 0.3433 - acc: 0.8469 - val_loss: 0.4277 - val_acc: 0.8380\nEpoch 77/100\n712/712 [==============================] - 0s 132us/step - loss: 0.3570 - acc: 0.8455 - val_loss: 0.4544 - val_acc: 0.8324\nEpoch 78/100\n712/712 [==============================] - 0s 128us/step - loss: 0.3678 - acc: 0.8385 - val_loss: 0.4356 - val_acc: 0.8324\nEpoch 79/100\n712/712 [==============================] - 0s 115us/step - loss: 0.3462 - acc: 0.8525 - val_loss: 0.4380 - val_acc: 0.8380\nEpoch 80/100\n712/712 [==============================] - 0s 124us/step - loss: 0.3381 - acc: 0.8469 - val_loss: 0.4578 - val_acc: 0.8324\nEpoch 81/100\n712/712 [==============================] - 0s 127us/step - loss: 0.3396 - acc: 0.8553 - val_loss: 0.4350 - val_acc: 0.8268\nEpoch 82/100\n712/712 [==============================] - 0s 126us/step - loss: 0.3410 - acc: 0.8455 - val_loss: 0.4507 - val_acc: 0.8436\nEpoch 83/100\n712/712 [==============================] - 0s 126us/step - loss: 0.3450 - acc: 0.8539 - val_loss: 0.4457 - val_acc: 0.8212\nEpoch 84/100\n712/712 [==============================] - 0s 123us/step - loss: 0.3432 - acc: 0.8553 - val_loss: 0.4779 - val_acc: 0.8268\nEpoch 85/100\n712/712 [==============================] - 0s 124us/step - loss: 0.3321 - acc: 0.8511 - val_loss: 0.5041 - val_acc: 0.8268\nEpoch 86/100\n712/712 [==============================] - 0s 121us/step - loss: 0.3413 - acc: 0.8553 - val_loss: 0.4531 - val_acc: 0.8268\nEpoch 87/100\n712/712 [==============================] - 0s 126us/step - loss: 0.3274 - acc: 0.8567 - val_loss: 0.4597 - val_acc: 0.8380\nEpoch 88/100\n712/712 [==============================] - 0s 121us/step - loss: 0.3293 - acc: 0.8511 - val_loss: 0.4763 - val_acc: 0.8212\nEpoch 89/100\n712/712 [==============================] - 0s 122us/step - loss: 0.3256 - acc: 0.8469 - val_loss: 0.4878 - val_acc: 0.8212\nEpoch 90/100\n712/712 [==============================] - 0s 125us/step - loss: 0.3300 - acc: 0.8553 - val_loss: 0.4612 - val_acc: 0.8156\nEpoch 91/100\n712/712 [==============================] - 0s 125us/step - loss: 0.3371 - acc: 0.8469 - val_loss: 0.4821 - val_acc: 0.8268\nEpoch 92/100\n712/712 [==============================] - 0s 127us/step - loss: 0.3446 - acc: 0.8511 - val_loss: 0.4812 - val_acc: 0.8380\nEpoch 93/100\n712/712 [==============================] - 0s 124us/step - loss: 0.3445 - acc: 0.8455 - val_loss: 0.4954 - val_acc: 0.8324\nEpoch 94/100\n712/712 [==============================] - 0s 128us/step - loss: 0.3311 - acc: 0.8553 - val_loss: 0.5003 - val_acc: 0.8324\nEpoch 95/100\n712/712 [==============================] - 0s 124us/step - loss: 0.3277 - acc: 0.8596 - val_loss: 0.4790 - val_acc: 0.8156\nEpoch 96/100\n712/712 [==============================] - 0s 123us/step - loss: 0.3174 - acc: 0.8596 - val_loss: 0.5112 - val_acc: 0.8324\nEpoch 97/100\n712/712 [==============================] - 0s 132us/step - loss: 0.3277 - acc: 0.8567 - val_loss: 0.4961 - val_acc: 0.8212\nEpoch 98/100\n712/712 [==============================] - 0s 131us/step - loss: 0.3229 - acc: 0.8511 - val_loss: 0.5701 - val_acc: 0.8268\nEpoch 99/100\n712/712 [==============================] - 0s 127us/step - loss: 0.3342 - acc: 0.8483 - val_loss: 0.5304 - val_acc: 0.8212\nEpoch 100/100\n712/712 [==============================] - 0s 128us/step - loss: 0.3322 - acc: 0.8497 - val_loss: 0.5554 - val_acc: 0.8324`}\n</Highlight>\n</details>\n\n<h3>5. 可視化</h3>\n<p>\n  Historyオブジェクトには、トレーニング経過の記録（history）や、モデル（model）、トレーニングに用いたハイパーパラメーター（params）などが格納されています。これを用いて、トレーニング経過の記録を可視化します。\n</p>\n<p>\n  History のうち、history を pandas の DataFrame にして、<a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html\" target=\"_blank\" rel=\"noopener noreferrer\">plot</a>関数でプロットしてみます。この関数は裏で<a href=\"https://matplotlib.org/\" target=\"_blank\" rel=\"noopener noreferrer\">matplotlib</a> を使っているため、使うにはmatplotlibをインストールした上で、Jupyter Notebook で使う際にはマジックコマンド（%matplotlib inline）を実行します。\n</p>\n<Highlight className=\"bash\">pipenv install matplotlib</Highlight>\n\n<Highlight className=\"python\">\n{`%matplotlib inline\nresults = pd.DataFrame(history.history)\nresults.head(2)`}</Highlight>\n<div style={{overflowX: \"auto\"}}>\n<table className=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th>val_loss</th>\n      <th>val_acc</th>\n      <th>loss</th>\n      <th>acc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.680474</td>\n      <td>0.385475</td>\n      <td>0.979209</td>\n      <td>0.528090</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.656121</td>\n      <td>0.687151</td>\n      <td>0.696029</td>\n      <td>0.558989</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n<p>損失関数のプロット。過学習気味のようです。</p>\n<Highlight className=\"python\">results[[\"loss\", \"val_loss\"]].plot()</Highlight>\n<div style={{overflowX: \"auto\"}}><img src={loss} alt=\"loss plot\"/></div>\n\n<p>精度のプロット。二値分類で検証データに対しておよそ８割の精度が出ているようです。</p>\n<Highlight className=\"python\">results[[\"acc\", \"val_acc\"]].plot()</Highlight>\n<div style={{overflowX: \"auto\"}}><img src={acc} alt=\"accuracy plot\"/></div>\n\n<p>\n  また、埋め込み層を可視化することもできます。方法は、<a href=\"http://projector.tensorflow.org/\" target=\"_blank\" rel=\"noopener noreferrer\">Embedding Projector プロジェクトページ</a>に書いてあります。\n</p>\n\n<h3>おわりに</h3>\n<p>Entity Embeddingを使うと、カテゴリ変数がメインのSQLテーブルやエクセルなどのデータから学習する深層学習モデルを作ることができます。</p>\n<p>ただし、この手法が必ず良い結果になるとは限りません。実際上記のモデルのKaggleでのsubmission scoreはおよそ0.7で、非常に悪いです。一方で、<a href=\"https://scikit-learn.org/stable/\" target=\"_blank\" rel=\"noopener noreferrer\">scikit-learn</a>の<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\" target=\"_blank\" rel=\"noopener noreferrer\">RandomForestRegressor</a>と<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\" target=\"_blank\" rel=\"noopener noreferrer\">GridSearchCV</a>を使った簡単な方法でおよそ0.8強の精度（上位10%ライン）が出ます。</p>\n<p>Entity Embeddingがもっとも光るのは、以下のケースでは無いかと思っています。</p>\n<ul>\n  <li>データが膨大すぎて、決定木などのナイーブな方法ではモデルが大きくなり過ぎる等対処ができないとき。</li>\n  <li>データに関するドメイン知識が無いことから、Feature Engineeringがあまりできないとき。</li>\n</ul>\n<p>\n  Kaggleのコンペは構造化データを扱うコンペが多いため、Entity Embeddingを使った解法を作ることができる場合が多いと思います。しかし、ランダムフォレストやSVMなどの方が精度が出る可能性もあることを認識しておくのは大事だと思います。\n</p>\n<p>\n  もし上記が間違っている、他のパターンの使い道がある、などあればぜひコメント欄で教えてください、と言いたいところですが、現在実装前のためよかったら<a href=\"https://twitter.com/KazuyaHatta\" target=\"_blank\" rel=\"noopener noreferrer\">Twitter</a>で教えてください。\n</p>\n</>\n    );\n  }\n}"],"sourceRoot":""}