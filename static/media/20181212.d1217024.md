
### Titanic (GridSearchCV + SMOTENC + RandomForest)

KaggleのチュートリアルのTitanicデータはちょっと非生存者のデータ数の方が多い。  
このデータは学習モデルの生存者と非生存者の割合を揃えると、モデルの予測精度が上がる。  
生存者のデータかさ増しをするにはSMOTEを使うことができる。  

まずはデータを読み込んで前処理をする。  
今回はよくある前処理を実施。あまりファンシーなことはしないです。


```python
import pandas as pd

train = pd.read_csv("./train.csv")
test = pd.read_csv("./test.csv")


y = train[["PassengerId", "Survived"]].copy() # よけておく

train.drop(columns="Survived", inplace=True) # testと形を揃える
combined = pd.concat([train, test]).reset_index(drop=True) # 前処理をまとめてやる為
```


```python
# 年齢とチケットの額のNull値を平均値で埋める
for col in ["Age", "Fare"]:
    combined[col].fillna(combined[col].mean(), inplace=True)

# キャビンがNullの箇所に文字を入れる
combined["Cabin"] = combined["Cabin"].str[0].fillna("NAN").astype("category").cat.as_ordered()

# 寄港地のNull値は一番多い「S」を代入
combined["Embarked"] = combined["Embarked"].fillna("S").astype("category").cat.as_ordered()

# 家族サイズを兄弟（SibSp）と親子（Parch）の人数の合算＋１（自分）で計算
combined["GroupSize"] = combined["SibSp"] + combined["Parch"] + 1

# 性別はとりあえずpandasのカテゴリー形式にしておく
combined["Sex"] = combined["Sex"].astype("category").cat.as_ordered()

# ここの手法をそのまま使用
# https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/
import numpy as np

def substrings_in_string(big_string, substrings):
    for substring in substrings:
        if big_string.find(substring) != -1:
            return substring
    return np.nan

title_list=[
    "Mrs",
    "Mr",
    "Master",
    "Miss",
    "Major",
    "Rev",
    "Dr",
    "Ms",
    "Mlle",
    "Col",
    "Capt",
    "Mme",
    "Countess",
    "Don",
    "Jonkhee"
]

combined['Title'] = combined['Name'].map(lambda x: substrings_in_string(x, title_list))

def replace_titles(x):
    title = x['Title']
    if   title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']: return 'Mr'
    elif title in ['Countess', 'Mme']:                                return 'Mrs'
    elif title in ['Mlle', 'Ms']:                                     return 'Miss'
    elif title =='Dr':
        if x['Sex']=='Male':                                          return 'Mr'
        else:                                                         return 'Mrs'
    else:
        return title

combined['Title'] = combined.apply(replace_titles, axis=1).astype("category").cat.as_ordered()

# 使わないカラムを落とす
combined.drop(columns=["Name", "Ticket", "SibSp", "Parch"], inplace=True)

# カテゴリコード化
sex, sex_cat = pd.factorize(combined["Sex"])
cabin, cabin_cat = pd.factorize(combined["Cabin"])
embarked, embarked_cat = pd.factorize(combined["Embarked"])
title, title_cat = pd.factorize(combined["Title"])

# 全てを一つにまとめる
full = pd.concat([
    combined[[
        "PassengerId",
        "Pclass",
        "Age",
        "Fare",
        "GroupSize"
    ]],
    pd.DataFrame({
        "Sex": sex,
        "Cabin": cabin,
        "Embarked": embarked,
        "Title": title
    })
], axis=1)
```


```python
full.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PassengerId</th>
      <th>Pclass</th>
      <th>Age</th>
      <th>Fare</th>
      <th>GroupSize</th>
      <th>Sex</th>
      <th>Cabin</th>
      <th>Embarked</th>
      <th>Title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>3</td>
      <td>22.0</td>
      <td>7.2500</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>1</td>
      <td>38.0</td>
      <td>71.2833</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>3</td>
      <td>26.0</td>
      <td>7.9250</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>1</td>
      <td>35.0</td>
      <td>53.1000</td>
      <td>2</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>3</td>
      <td>35.0</td>
      <td>8.0500</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
# トレーニング用データとテスト用データに戻す。
preprocessed_train = full.loc[full["PassengerId"].isin(train["PassengerId"])]
preprocessed_test = full.loc[full["PassengerId"].isin(test["PassengerId"])]
```

### GridSearchCV

Scikit-learnでRandomForestの予測モデルを作るときは、ハイパーパラメーターの自動チューニングにGridSearchCVを使うことができます。  
Scikit-learnのページで紹介されてたままで使用。


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

# 施行するハイパーパラメーター一覧
param_grid = {
    "criterion" : ["gini", "entropy"],
    "min_samples_leaf" : [1, 5, 10, 25, 50, 70],
    "min_samples_split" : [2, 4, 10, 12, 16, 18, 25, 35],
    "n_estimators": [100, 400, 700, 1000, 1500]
}

# 予測モデルの初期化
m = RandomForestClassifier(
    n_estimators=100, max_features="auto", oob_score=True, random_state=1, n_jobs=-1
)

# GridSearchCVの初期化。予測モデルのインスタンスを渡す。
clf = GridSearchCV(estimator=m, param_grid=param_grid, n_jobs=-1)
```


```python
# トレーニング用データ全体でハイパーパラメーター探索をする。データとしては必要のない乗客IDを消す。
X = preprocessed_train.drop(columns="PassengerId")
Y = y["Survived"]

# 探索開始
clf.fit(X, Y)
```

    /Users/hattakazuya/.local/share/virtualenvs/titanic-sq0fNzuf/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.
      warnings.warn(CV_WARNING, FutureWarning)





    GridSearchCV(cv='warn', error_score='raise-deprecating',
           estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=None, max_features='auto', max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=1, min_samples_split=2,
                min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                oob_score=True, random_state=1, verbose=0, warm_start=False),
           fit_params=None, iid='warn', n_jobs=-1,
           param_grid={'criterion': ['gini', 'entropy'], 'min_samples_leaf': [1, 5, 10, 25, 50, 70], 'min_samples_split': [2, 4, 10, 12, 16, 18, 25, 35], 'n_estimators': [100, 400, 700, 1000, 1500]},
           pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
           scoring=None, verbose=0)



終わるのにラップトップで15分くらいかかる。  
どの組み合わせがよかったかはbest_params_メソッドで見れる。


```python
clf.best_params_
```




    {'criterion': 'gini',
     'min_samples_leaf': 1,
     'min_samples_split': 12,
     'n_estimators': 100}



実際にこのモデルでトレーニングをしてみる。  
ここで、SMOTEが出てきます。

### SMOTE / SMOTENC

ラベルに偏りのあるデータのマイノリティ側のデータを生成して、ラベルに偏りが出ないようにする手法。  
ただ複製するだけではない点に注意。k近傍法を使って似たデータを生成する。  
もともとSMOTEは入力が連続値データの時に使われる手法だったのに対して、SMOTENCはカテゴリ値も同時に扱えるようにしたSMOTEの派生。

SMOTEをするときは、まずトレーニングデータを訓練データと検証データに分けます。  
これは、トレーニングデータ全体にSMOTEをかけて、それから訓練データと検証データに分けると、両方に同じデータが含まれてしまう可能性が出る為。


```python
# おおよそ 8:2 で分ける。乗客IDをランダムにシャッフルして区切り位置で分ける
split = round(preprocessed_train.shape[0] * 0.8)
pids = preprocessed_train["PassengerId"].values
rand_pids = np.random.permutation(pids)
train_pids = sorted(rand_pids[:split])
valid_pids = sorted(rand_pids[split:])

# 訓練データとテストデータ
trainX = preprocessed_train.loc[preprocessed_train["PassengerId"].isin(train_pids)].values[:, 1:]
validX = preprocessed_train.loc[preprocessed_train["PassengerId"].isin(valid_pids)].values[:, 1:]
trainY = y.loc[y["PassengerId"].isin(train_pids), "Survived"].values
validY = y.loc[y["PassengerId"].isin(valid_pids), "Survived"].values
```

SMOTE（今回はSMOTENC）を使うには、imbalanced-learnパッケージ（scikit-learnのcontribパッケージ）を使います。


```python
from imblearn.over_sampling import SMOTENC
```

SMOTENCではカテゴリ値とそうでない値をカラム位置で指定することができます。


```python
sm = SMOTENC(categorical_features=[0, 4, 5, 6, 7], random_state=0)
```

sckikit-learnとAPIは似ています。fit_resampleメソッドを使って、クラスのバランスを揃えたデータを返します。  
ここで、そもそものデータはどうなっているのか見てみます。


```python
f"生存：{(trainY==1).sum()}, 非生存：{(trainY==0).sum()}"
```




    '生存：283, 非生存：430'




```python
430/283
```




    1.519434628975265



0が非生存者、1が生存者なので、非生存者が5割くらい多いデータです。  
fit_resampleをかけてみます。


```python
resampledX, resampledY = sm.fit_resample(trainX, trainY)
```


```python
f"生存：{(resampledY==1).sum()}, 非生存：{(resampledY==0).sum()}"
```




    '生存：430, 非生存：430'



両方430に増えていることがわかります。  
入力の方はk近傍法で自動生成されています。  

先ほど計算したRandomForestのベストモデルとこのデータを用いてトレーニングしてみます。


```python
m = clf.best_estimator_  # ベストモデルを取得する関数
m.fit(resampled_X, resampled_y)
```




    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=None, max_features='auto', max_leaf_nodes=None,
                min_impurity_decrease=0.0, min_impurity_split=None,
                min_samples_leaf=1, min_samples_split=12,
                min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,
                oob_score=True, random_state=1, verbose=0, warm_start=False)



fast.ai で使われていたスコア出力関数で精度をみてみます。


```python
import math
def rmse(x,y): return math.sqrt(((x-y)**2).mean())

def print_score(m, X_train, y_train, X_valid, y_valid):
    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),
                m.score(X_train, y_train), m.score(X_valid, y_valid)]
    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)
    print(res)
```


```python
print_score(m, resampled_X, resampled_y, validX, validY)
```

    [0.3198836997962679, 0.3821876708246056, 0.8976744186046511, 0.8539325842696629, 0.8302325581395349]


ちょっと過学習気味のようです。  
Kaggleにサブミットしてみます。


```python
# テストデータで予測
pred = m.predict(preprocessed_test.iloc[:, 1:].values)

# 予測結果を乗客IDと組み合わせて提出用CSVを作成
result = pd.DataFrame({
    "PassengerId": preprocessed_test["PassengerId"],
    "Survived": pred
})

result.to_csv("res_1212.csv", index=False)
```

結果は 0.8032 で全体順位は1191、割合としてはTop12%でした。